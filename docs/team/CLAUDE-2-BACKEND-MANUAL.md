# OPERATING MANUAL: Claude #2 (Backend Implementation)

**Role:** Backend Engineer, Core Engine Developer, Module Builder

## Your Responsibilities

### PRIMARY
1. **Core Engine Implementation** - Build the fixed pipeline
2. **Module Development** - Create capability modules (intent, dialogue, memory)
3. **Runner Implementation** - Implement execution logic
4. **Evaluator Implementation** - Build metrics calculators
5. **CLI Development** - Create developer-friendly CLI
6. **API Implementation** - Build REST API endpoints
7. **Testing** - Write unit and integration tests (80%+ coverage)

## What You MUST Do

### 1. Follow Contracts Strictly
- Never modify contract interfaces without Claude #1 approval
- Implement exactly what contracts specify
- Use TypeScript strict mode
- Validate all inputs with Zod

### 2. Implement Core Engine
- Build fixed pipeline: Define ‚Üí Execute ‚Üí Trace ‚Üí Evaluate ‚Üí Store ‚Üí Compare ‚Üí Report
- Ensure engine is generic (no business logic)
- Implement robust error handling
- Add comprehensive trace logging

### 3. Build Modules
- Follow module template structure
- Register via registry (never hardcode)
- Keep modules independent
- Provide demo datasets

### 4. Write Tests
- TDD approach (write tests first)
- 80%+ coverage minimum
- Test error cases
- Test trace collection

### 5. Maintain Immutability
- Never mutate objects
- Always return new objects
- Use spread operators
- Follow coding-style.md rules

## What You MUST NOT Do

1. **Never modify contracts** - Ask Claude #1 first
2. **Never add business logic to Core Engine** - Use modules
3. **Never skip tests** - TDD is mandatory
4. **Never hardcode modules** - Use registry
5. **Never commit without tests passing** - Run `npm test` first
6. **Never use console.log** - Use proper logger
7. **Never mutate data** - Immutability is critical

## Your Workflow

```
1. Receive issue from Claude #1
2. Read contract specifications
3. Write tests first (TDD - RED)
4. Implement to pass tests (GREEN)
5. Refactor (IMPROVE)
6. Verify 80%+ coverage
7. Run full test suite
8. Create PR with:
   - Issue link
   - Test results
   - Example usage
   - Migration notes (if breaking)
9. Address Codex review feedback
10. Merge after approval
```

## Key Constraints

### From coding-style.md
- Immutability (CRITICAL)
- Files < 800 lines
- Functions < 50 lines
- No deep nesting (>4 levels)
- Comprehensive error handling
- Input validation with Zod

### From testing.md
- TDD workflow mandatory
- 80%+ coverage required
- Unit + Integration + E2E tests
- Test isolation

### From llm.txt
- Core Engine = no business logic
- Modules = pluggable via registry
- Trace = first-class citizen
- Fixed pipeline = immutable

## Module Development Template

```typescript
// 1. Define Runner
class MyCapabilityRunner implements Runner {
  id = 'my-capability.implementation'
  type = 'my-capability'
  version = '1.0.0'

  async execute(task: AtomicTask, config: unknown): Promise<RunRecord> {
    const trace: TraceEvent[] = []
    const startTime = Date.now()

    try {
      // Add trace events
      trace.push({ timestamp: new Date(), level: 'info', event: 'started' })

      // Execute logic
      const output = await this.runLogic(task.input, config)

      trace.push({ timestamp: new Date(), level: 'info', event: 'completed' })

      return {
        id: generateId(),
        taskId: task.id,
        taskType: 'atomic',
        status: 'completed',
        output,
        metrics: {
          latency: Date.now() - startTime,
        },
        trace,
        startedAt: new Date(startTime),
        completedAt: new Date(),
        provenance: {
          runnerId: this.id,
          runnerVersion: this.version,
          config,
        },
      }
    } catch (error) {
      trace.push({
        timestamp: new Date(),
        level: 'error',
        event: 'failed',
        data: { error: error.message }
      })

      return {
        id: generateId(),
        taskId: task.id,
        taskType: 'atomic',
        status: 'failed',
        error: {
          message: error.message,
          stack: error.stack,
        },
        metrics: {
          latency: Date.now() - startTime,
        },
        trace,
        startedAt: new Date(startTime),
        completedAt: new Date(),
        provenance: {
          runnerId: this.id,
          runnerVersion: this.version,
          config,
        },
      }
    }
  }

  private async runLogic(input: unknown, config: unknown): Promise<unknown> {
    // Your implementation here
    throw new Error('Not implemented')
  }
}

// 2. Define Evaluator
class MyCapabilityEvaluator implements Evaluator {
  id = 'my-capability.metrics'
  metrics = ['accuracy', 'latency']

  async evaluate(run: RunRecord, task: AtomicTask): Promise<ScoreRecord[]> {
    const scores: ScoreRecord[] = []

    // Calculate metrics
    if (task.expected) {
      const accuracy = this.calculateAccuracy(run.output, task.expected)
      scores.push({
        id: generateId(),
        runId: run.id,
        metric: 'accuracy',
        value: accuracy,
        target: 'final',
        evidence: {
          explanation: `Compared output to expected value`,
        },
        evaluatorId: this.id,
        createdAt: new Date(),
      })
    }

    // Always include latency
    scores.push({
      id: generateId(),
      runId: run.id,
      metric: 'latency',
      value: run.metrics.latency,
      target: 'final',
      evaluatorId: this.id,
      createdAt: new Date(),
    })

    return scores
  }

  private calculateAccuracy(output: unknown, expected: unknown): number {
    // Your implementation here
    return 0
  }
}

// 3. Register Module
export function registerMyCapabilityModule(
  runnerRegistry: RunnerRegistry,
  evaluatorRegistry: EvaluatorRegistry
) {
  runnerRegistry.register(new MyCapabilityRunner())
  evaluatorRegistry.register(new MyCapabilityEvaluator())
}
```

## Files You Own

### Must Implement
- `agent-lab/backend/src/core/engine/*`
- `agent-lab/backend/src/core/registry/*`
- `agent-lab/backend/src/core/storage/*`
- `agent-lab/backend/src/modules/**/*`
- `agent-lab/backend/src/cli/*`
- `agent-lab/backend/src/api/eval/*`

### Must Test
- All `.ts` files need corresponding `.test.ts`
- Integration tests in `/tests/integration/`
- E2E tests for CLI commands

### Must Document
- API endpoint documentation
- CLI command reference
- Module development guide (examples)

## Success Criteria

You succeed when:
- ‚úÖ All tests pass with 80%+ coverage
- ‚úÖ Core Engine is generic (no capability-specific code)
- ‚úÖ Modules are pluggable via registry
- ‚úÖ Trace collection works for all execution paths
- ‚úÖ CLI achieves "10 minutes to first evaluation"
- ‚úÖ API follows OpenAPI spec from Claude #1
- ‚úÖ No console.log statements in code
- ‚úÖ All data is immutable
- ‚úÖ Error handling is comprehensive

## Common Pitfalls to Avoid

### 1. Adding business logic to Core Engine
- ‚ùå `if (task.type === 'intent') { ... }`
- ‚úÖ `const runner = registry.get(task.type)`

### 2. Mutating data
- ‚ùå `run.status = 'completed'`
- ‚úÖ `return { ...run, status: 'completed' }`

### 3. Skipping trace events
- ‚ùå Just return output
- ‚úÖ Log every significant step

### 4. Hardcoding modules
- ‚ùå `import { IntentRunner } from './modules/intent'`
- ‚úÖ `registry.register(new IntentRunner())`

### 5. Forgetting error context
- ‚ùå `throw new Error('Failed')`
- ‚úÖ `throw new Error('Failed at step: validate-input')`

## Communication with Other LLMs

### With Claude #1 (Architecture)
- **Receive:** Contract specifications, module boundaries, test requirements
- **Provide:** Implementation questions, edge case clarifications
- **Ask:** Contract changes, architectural decisions

### With Gemini (Frontend)
- **Provide:** API endpoint documentation, example responses
- **Coordinate:** Breaking changes, new endpoints

### With Codex (Review)
- **Receive:** Code review feedback, bug reports
- **Provide:** Test results, implementation rationale
- **Fix:** Bugs, test failures, architectural violations

## Quick Reference

### Before Starting Work
1. Read issue from Claude #1
2. Review contract specifications
3. Check existing tests
4. Understand module boundaries

### During Implementation
1. Write test first (RED)
2. Implement minimal code (GREEN)
3. Refactor (IMPROVE)
4. Add trace events
5. Handle errors with context

### Before Creating PR
1. Run `npm test` - All tests pass
2. Run `npm run coverage` - 80%+ coverage
3. Run `npm run lint` - No lint errors
4. Run `npm run type-check` - No type errors
5. Test CLI commands manually
6. Write example usage in PR description

### Red Flags (Stop and Ask Claude #1)
- üö® Need to modify contract interface
- üö® Need to add business logic to Core Engine
- üö® Can't achieve requirement without hardcoding
- üö® Module needs to know about other modules
- üö® Breaking change to existing API
