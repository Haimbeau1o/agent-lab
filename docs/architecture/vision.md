# Agent Lab - 项目愿景

## 一句话定义

**Agent Lab = 面向开发者的 Agent 能力实验台 + 标准化评测运行时（Eval Runtime）**

用于以**可复现、可对比、可解释**的方式评估 Agent 的**单能力**和**多能力组合链路（Scenario）**。

## 核心用户问题

开发者在做 Agent 时，无法可靠地：

1. **验证能力是否真的变好了** - 缺乏标准化评测方法
2. **对比不同实现** - 同一能力的多种实现（模型/prompt/框架/本地服务）无法横向对比
3. **调试多步行为** - 工具调用、记忆读写、多轮对话的执行过程不透明
4. **复现实验结果** - 配置、数据、代码版本漂移导致结果不可复现

## 解决方案

Agent Lab 提供：

### 1. 标准化评测运行时
- 固定的评测流程（Pipeline）
- 统一的任务定义（Task Contract）
- 完整的执行追踪（Trace）
- 可扩展的评估器（Evaluator）

### 2. 能力模块化
- 原子能力评测（AtomicTask）：单一能力的独立测试
- 组合链路评测（ScenarioTask）：多能力协同的端到端测试
- 插件化架构：新能力通过注册机制接入，无需修改核心

### 3. 可复现性保证
- 完整的执行记录（RunRecord）
- 版本化的配置（Provenance）
- 数据集管理
- A/B 对比与回归测试

## 产品边界（长期坚持）

### 我们做什么
✅ 评测与验证基础设施
✅ 原子能力评测（AtomicTask）
✅ 组合链路评测（ScenarioTask / 多步 Pipeline）
✅ A/B 对比与回归评测
✅ 内置指标 + 用户自定义 Evaluator
✅ Trace / 可解释 / 可复现

### 我们不做什么
❌ 生产 Agent 托管与编排平台
❌ 企业级权限 / 多租户 / IAM（P1+ 才考虑）
❌ 全流程自动化业务系统

## P0 目标

> **开发者 10 分钟内跑出一次评测并拿到可复现结果**

### 用户旅程
```bash
# 1. 安装（2分钟）
npm install -g agent-lab

# 2. 配置（2分钟）
agent-lab init
agent-lab config set OPENAI_API_KEY=xxx

# 3. 运行示例（1分钟）
agent-lab run --demo intent

# 4. 查看结果（5分钟）
agent-lab report --run <id>
```

## 成功指标

### 技术指标
- 新能力接入 < 1 天（通过注册机制）
- 评测执行时间 < 5 秒（单个 AtomicTask）
- Trace 完整性 100%（所有执行步骤可追溯）
- 结果可复现率 100%（相同配置相同结果）

### 用户指标
- 首次评测时间 < 10 分钟
- 文档完整度 > 90%
- 社区贡献模块数 > 5（6个月内）

## 长期愿景

当项目成熟时：
- 新能力 = 新模块目录
- 新实现 = 新 Runner 文件
- 新指标 = 新 Evaluator 文件
- 不需要修改 Engine
- 不需要改已有模块

**价值主张：可复现、可对比、可解释、可扩展的 Agent 工程验证体系**
